{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6dda53e-8a12-4d99-a093-769959e2f226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\\nrepresent?'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5ad9558e-4f6b-47e9-ab80-cf34c4ee6f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  ## before calculating thge r-square we have to calculate many things\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "da47b13b-f37b-4a7b-99a8-e71b763f176b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weight</th>\n",
       "      <th>Height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Weight  Height\n",
       "0      45     120\n",
       "1      58     135\n",
       "2      48     123\n",
       "3      60     145\n",
       "4      70     160"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('height-weight.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "26a426c1-3e7f-4081-9dc6-1ecb6abc9e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23 entries, 0 to 22\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   Weight  23 non-null     int64\n",
      " 1   Height  23 non-null     int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 496.0 bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ae3950cd-8ab8-4c40-b4e0-bfc30001b5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df[['Weight']] #independent feature\n",
    "y=df[['Height']] #dependent feature                ## dividing into dependent and independent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cbb07b38-526a-4d47-b67e-d329a45405ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23, 1), (23, 1))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c812b3cc-18d9-483c-a61e-2f301948b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e11b82b6-a3a8-42e9-8609-1e5b19ab0fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "42805c6f-7d69-407b-9d78-4f96985b5282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15, 1), (8, 1), (15, 1), (8, 1))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "868aee39-136b-4723-8710-0bdb2ffb9ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dbaf30dc-a86f-429c-8fee-b97391e2ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler =  StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aa0fd178-8bba-4fe3-8701-8cb54fc9e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train =scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "924484aa-2983-4ae9-b61b-10b5916265e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a8a514e7-6d31-4406-adcb-45b469d639d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.41007671],\n",
       "       [-1.51215787],\n",
       "       [ 1.49934297],\n",
       "       [-1.70438132],\n",
       "       [-0.74326404],\n",
       "       [-0.10251918],\n",
       "       [ 0.28192774],\n",
       "       [-1.3840089 ],\n",
       "       [-0.99956198],\n",
       "       [ 0.02562979],\n",
       "       [ 1.17897054],\n",
       "       [ 0.66637465],\n",
       "       [ 0.85859811],\n",
       "       [ 0.98674708],\n",
       "       [ 0.53822568]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3bd76c61-f74a-468e-800a-4bb609dfaf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9fac4049-5d8e-40c0-9327-b1224ac7fffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor =  LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "47145c7d-b604-4b69-abb0-2cff041d3992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "55e543c1-ae99-42c7-b368-56678c133497",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "91fd50bb-9a52-4b8d-b384-f51e49d14da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[162.8051967 ],\n",
       "       [162.8051967 ],\n",
       "       [128.40340121],\n",
       "       [180.52733377],\n",
       "       [149.25297424],\n",
       "       [190.95212028],\n",
       "       [141.95562368],\n",
       "       [185.73972703]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "49dc9e9a-d0c4-4799-b36a-5744f5061a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7870211405217767"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## now we w'll calculate the r_square test\n",
    "from sklearn.metrics import r2_score\n",
    "score = r2_score(y_pred_test,y_test) ## here i have caluculated the r_square test\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8caa23-876f-428e-862a-2f4374c0f2ae",
   "metadata": {},
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5607ff6-bd62-4da0-a45f-84c881d75d5e",
   "metadata": {},
   "source": [
    "R square\n",
    "Formula\n",
    "\n",
    "R^2 = 1 - SSR/SST\n",
    "\n",
    "R^2 = Accuracy of the model\n",
    "SSR = sum of squares of residuals   ## difference between r_square and adjusted r_square\n",
    "SST = total sum of squares\n",
    "\n",
    "Adjusted r square\n",
    "Adjusted R2 = 1 – [(1-R2)*(n-1)/(n-k-1)]\n",
    "\n",
    "where:\n",
    "\n",
    "R2: The R2 of the model\n",
    "n: The number of observations\n",
    "k: The number of predictor variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "01690567-c43c-4ca6-87b4-630caf2e1fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7515246639420728"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - (1-score)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1) ## score=r_square #n=y_test,k=x_ttest\n",
    "##adjusted r_square"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaa2da4-8e59-42d0-804a-abce265d0e14",
   "metadata": {},
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def75753-24a6-4364-b4c3-ccda0187425a",
   "metadata": {},
   "source": [
    "Adjusted R-squared is more appropriate to use when you are dealing with multiple regression models, where you have more than one independent variable. It's particularly useful in situations where you want to compare the goodness of fit between models with different numbers of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a18e2bc-478f-426e-94b1-435bdc6eed35",
   "metadata": {},
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "45a434b0-50b5-42d4-9f16-be60c9a763fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error,mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "beafedac-06eb-4083-a7b6-6b13ea43a8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE= mean_squared_error(y_test,y_pred_test) ## we can calculate rmse,mse ,mae like this\n",
    "MAE=mean_absolute_error(y_test,y_pred_test)\n",
    "RMSE=np.sqrt(MSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "05676b75-cc89-4434-b297-0c26c4519399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91.42562383241722, 8.332521348806658, 9.561674739940552)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE,MAE,RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7fa8d7-06cc-49f5-b499-91922dd4b356",
   "metadata": {},
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6b77ba-930e-4f3d-b1ee-8647d31e56ba",
   "metadata": {},
   "source": [
    "Root Mean Squared Error (RMSE):\n",
    "Advantages:\n",
    "\n",
    "Sensitive to Large Errors: RMSE gives more weight to large errors. This can be useful in situations where large errors are particularly costly or impactful.\n",
    "\n",
    "Differentiable: RMSE is differentiable, which makes it useful for gradient-based optimization algorithms, such as those used in training neural networks.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Sensitive to Outliers: RMSE is sensitive to outliers because it involves squaring the errors. A single large error can significantly inflate the RMSE.\n",
    "\n",
    "Units of Measurement: The RMSE value is in the same units as the dependent variable, which can sometimes make it difficult to interpret in real-world terms.\n",
    "\n",
    "Mean Squared Error (MSE):\n",
    "Advantages:\n",
    "\n",
    "Mathematical Simplicity: MSE is simple to compute and understand, as it's just the average of the squared errors.\n",
    "\n",
    "Penalizes Large Errors: Like RMSE, MSE penalizes larger errors more heavily, which can be important in some contexts.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Units Squared: MSE is in squared units of the dependent variable, which can make it harder to interpret compared to MAE.\n",
    "\n",
    "Sensitivity to Outliers: Similar to RMSE, MSE is sensitive to outliers because it involves squaring the errors.\n",
    "\n",
    "Mean Absolute Error (MAE):\n",
    "Advantages:\n",
    "\n",
    "Robust to Outliers: MAE is less sensitive to outliers compared to MSE and RMSE. It provides a more balanced view of the errors.\n",
    "\n",
    "Interpretability: MAE is in the same units as the dependent variable, which makes it easier to interpret in practical terms.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Less Sensitive to Large Errors: MAE treats all errors equally. In situations where large errors are of particular concern, MAE may not be the best choice.\n",
    "\n",
    "Not Differentiable at Zero: MAE is not differentiable at zero, which can be a consideration if you're using optimization algorithms that require differentiability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10bc615-4328-4ce0-9fbd-40540c219410",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4973f8-1629-44b0-8210-38abda016c0c",
   "metadata": {},
   "source": [
    "Lasso (Least Absolute Shrinkage and Selection Operator) and Ridge regularization are techniques used in linear regression to prevent overfitting and improve the generalization ability of the model. They achieve this by adding a penalty term to the loss function.\n",
    "When to Use Lasso vs. Ridge:\n",
    "Use Lasso When:\n",
    "\n",
    "You believe that only a subset of the features is important.\n",
    "You want to perform feature selection and potentially reduce the number of predictors.\n",
    "Use Ridge When:\n",
    "\n",
    "You believe that all features are relevant, but some may have high multicollinearity.\n",
    "You want to shrink coefficients but retain all features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbacd67-01ab-4bd4-9227-47182352f8ee",
   "metadata": {},
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467823a7-bf82-474c-bb46-582be35224f8",
   "metadata": {},
   "source": [
    "Regularized linear models, such as Ridge and Lasso regression, help prevent overfitting in machine learning by adding a penalty term to the model's loss function. This penalty discourages the model from assigning too much importance to any single feature, which can lead to overfitting.\n",
    "\n",
    "Let's break down how this works with an example:\n",
    "\n",
    "Example: Ridge Regression\n",
    "Suppose you're trying to predict housing prices based on various features like square footage, number of bedrooms, location, etc. You collect a dataset with these features and their corresponding prices.\n",
    "\n",
    "Without regularization, a linear regression model might try to fit the data as closely as possible, potentially assigning very high coefficients to some features. This can lead to a model that's too complex and fits the training data too closely, resulting in poor performance on new, unseen data (overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465e3ed8-a64b-4459-a7df-abdcb09e0f13",
   "metadata": {},
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02383849-3e28-47fb-94de-768ff656da82",
   "metadata": {},
   "source": [
    "Regularized linear models like Ridge and Lasso regression are powerful tools for preventing overfitting and improving the generalization of a model. However, they do have some limitations, and they may not always be the best choice for every regression analysis. Here are some of the limitations:\n",
    "\n",
    "Not Suitable for Non-Linear Relationships:\n",
    "\n",
    "Regularized linear models assume a linear relationship between the features and the target variable. If the true relationship is highly non-linear, these models may not perform well.\n",
    "May Remove Important Features (Lasso):\n",
    "\n",
    "Lasso regression can set some coefficients to exactly zero, effectively excluding those features from the model. If some of these features are actually important for prediction, Lasso may not be the best choice.\n",
    "May Not Handle Categorical Variables Well:\n",
    "\n",
    "Categorical variables with a large number of levels (high cardinality) can be challenging for regularized linear models. One-hot encoding these variables can lead to a large number of predictors, potentially causing over-regularization.\n",
    "Dependent on Proper Scaling of Features:\n",
    "\n",
    "Regularized linear models are sensitive to the scale of the features. If features are not properly scaled (e.g., some have much larger values than others), the model's performance may be suboptimal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c08257e-c9ec-41d9-bedd-02b5d99116c2",
   "metadata": {},
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d6e768-7425-4603-b164-3315cb48ddbf",
   "metadata": {},
   "source": [
    "Model B with an MAE of 8 is performing better in terms of average absolute error. This means that, on average, its predictions are closer to the actual values compared to Model A.\n",
    "\n",
    "However, it's important to consider the specific requirements of your problem. For example, if certain large errors are particularly costly or unacceptable, you might lean towards the model with the lower RMSE, as it puts more weight on those larger errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cead0e-f3c1-45a8-8ff1-9e0ddbe06d98",
   "metadata": {},
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d3e0f0-bd74-447b-aeba-799a3a52e658",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
